<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Working with imbalanced datasets • imbalance</title>
<!-- jquery --><script src="https://code.jquery.com/jquery-3.1.0.min.js" integrity="sha384-nrOSfDHtoPMzJHjVTdCopGqIqeYETSXhZDFyniQ8ZHcVy08QesyHcnOUpMpqnmWq" crossorigin="anonymous"></script><!-- Bootstrap --><link href="https://maxcdn.bootstrapcdn.com/bootswatch/3.3.7/cosmo/bootstrap.min.css" rel="stylesheet" crossorigin="anonymous">
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script><!-- Font Awesome icons --><link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" integrity="sha384-T8Gy5hrqNKT+hzMclPo118YTQO6cYprQmhrYwIiQ/3axmI1hQomh7Ud2hPOy8SP1" crossorigin="anonymous">
<!-- pkgdown --><link href="../pkgdown.css" rel="stylesheet">
<script src="../jquery.sticky-kit.min.js"></script><script src="../pkgdown.js"></script><!-- mathjax --><script src="https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body>
    <div class="container template-vignette">
      <header><div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="../index.html">imbalance</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li>
  <a href="../index.html">
    <span class="fa fa-home"></span>
     
    Home
  </a>
</li>
<li>
  <a href="../reference/index.html">
    <span class="fa fa-bars"></span>
     
    Quick reference
  </a>
</li>
<li>
  <a href="../articles/imbalance.html">
    <span class="fa fa-book"></span>
     
    Vignette
  </a>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right">
<li>
  <a href="https://github.com/ncordon/imbalance">
    <span class="fa fa-github"></span>
     
  </a>
</li>
      </ul>
</div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->

      
      </header><div class="row">
  <div class="col-md-9">
    <div class="page-header toc-ignore">
      <h1>Working with imbalanced datasets</h1>
                        <h4 class="author">Ignacio Cordón</h4>
            
          </div>

    
    
<div class="contents">
<div id="imbalance-classification-problem" class="section level1">
<h1 class="hasAnchor">
<a href="#imbalance-classification-problem" class="anchor"></a>Imbalance classification problem</h1>
<p>Let:</p>
<ul>
<li>
<span class="math inline">\(S=\{(x_1, y_1), \ldots (x_m, y_m)\}\)</span> be our training data for a classification problem, where <span class="math inline">\(y_i \in \{0,1\}\)</span> will be our data labels. Therefore, we will have a binary classification problem.</li>
<li>
<span class="math inline">\(S^{+} = \{(x,y) \in S: y=1\}\)</span> be the positive or minority instances.</li>
<li>
<span class="math inline">\(S^{-} = \{(x,y) \in S: y=-1\}\)</span> be the negative or majority instances.</li>
</ul>
<p>If <span class="math inline">\(|S^{+}| &gt; |S^{-}|\)</span>, the performance of classification algorithms is highly hindered, especially when it comes to the positive class. Therefore, methods to improve that performance are required.</p>
<p>Namely, <code>imbalance</code> package provides <em>oversampling</em> algorithms. Those family of procedures aim to generate a set <span class="math inline">\(E\)</span> of synthetic positive instances based on the training ones, so that we have a new classification problem with <span class="math inline">\(\bar{S}^{+} = S^{+} \cup E\)</span>, <span class="math inline">\(\bar{S}^{-} = S^{-}\)</span> and <span class="math inline">\(\bar{S} = \bar{S}^{+}\cup \bar{S}^{-}\)</span> our new training set.</p>
</div>
<div id="contents-of-the-package" class="section level1">
<h1 class="hasAnchor">
<a href="#contents-of-the-package" class="anchor"></a>Contents of the package</h1>
<p>In the package, we have the following <em>oversampling</em> functions available:</p>
<ul>
<li><code>mwmote</code></li>
<li><code>racog</code></li>
<li><code>wracog</code></li>
<li><code>rwo</code></li>
<li><code>pdfos</code></li>
</ul>
<p>Each of these functions can be applied to a binary dataset (that is, a set of data where labels <span class="math inline">\(y\)</span> could only take two possible values). In particular, the following examples will use datasets included in the package, which are imbalanced datasets. For example, we can run <code>pdfos</code> algorithm on <code>newthyroid1</code> dataset.</p>
<p>First of all we could check the shape of the dataset:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(<span class="st">"imbalance"</span>)
<span class="kw">data</span>(newthyroid1)

<span class="kw">head</span>(newthyroid1)</code></pre></div>
<pre><code>##   T3resin Thyroxin Triiodothyronine Thyroidstimulating TSH_value    Class
## 1     105      7.3              1.5                1.5      -0.1 negative
## 2      67     23.3              7.4                1.8      -0.6 positive
## 3     111      8.4              1.5                0.8       1.2 negative
## 4      89     14.3              4.1                0.5       0.2 positive
## 5     105      9.5              1.8                1.6       3.6 negative
## 6     110     20.3              3.7                0.6       0.2 positive</code></pre>
<p>Clearly, <code>Class</code> is the class attribute of the dataset and there are two possible classes: <code>positive</code> and <code>negative</code>. How many instances do we need to balance the dataset? We could easily compute this by doing:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">numPositive &lt;-<span class="st"> </span><span class="kw">length</span>(<span class="kw">which</span>(newthyroid1$Class ==<span class="st"> "positive"</span>))
numNegative &lt;-<span class="st"> </span><span class="kw">length</span>(<span class="kw">which</span>(newthyroid1$Class ==<span class="st"> "negative"</span>))
nInstances &lt;-<span class="st">  </span>numNegative -<span class="st"> </span>numPositive</code></pre></div>
<p>We get that we need to generate 145 instances to balance the dataset. It would not be advisable such a high number of instances, due to the scarcity of minority examples required to infer data structure. We could try to generate 80 synthetic examples instead:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">newSamples &lt;-<span class="st"> </span><span class="kw"><a href="../reference/pdfos.html">pdfos</a></span>(<span class="dt">dataset =</span> newthyroid1, <span class="dt">numInstances =</span> <span class="dv">80</span>, 
                    <span class="dt">classAttr =</span> <span class="st">"Class"</span>)</code></pre></div>
<p><code>newSamples</code> would contain the 80 synthetic examples, with same shape as the original dataset <code>newthyroid1</code>.</p>
<p>All of the algorithms can be used with the minimal parameters <code>dataset</code>, <code>numInstances</code> and <code>classAttr</code>, except for <code>wRACOG</code>, which does not have a <code>numInstances</code> parameter. The latter adjusts this number itself, and needs two datasets (more accurately, two partitions of the same dataset), <code>train</code> and <code>validation</code> to work.</p>
<p>The package also includes a method to plot a visual comparison between the oversampled dataset and the old imbalanced dataset:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Bind a balanced dataset</span>
newDataset &lt;-<span class="st"> </span><span class="kw">rbind</span>(newthyroid1, newSamples)
<span class="co"># Plot a visual comparison between new and old dataset</span>
<span class="kw"><a href="../reference/plotComparison.html">plotComparison</a></span>(newthyroid1, newDataset, 
               <span class="dt">attrs =</span> <span class="kw">names</span>(newthyroid1)[<span class="dv">1</span>:<span class="dv">3</span>], <span class="dt">classAttr =</span> <span class="st">"Class"</span>)</code></pre></div>
<p><img src="imbalance_files/figure-html/example-plot-1.png" width="960"></p>
<p>There is also a filtering algorithm available, <code>neater</code>, to cleanse synthetic instances. This algorithm could be used with every oversampling method, either included in this package or in another one:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">filteredSamples &lt;-<span class="st"> </span><span class="kw"><a href="../reference/neater.html">neater</a></span>(newthyroid1, newSamples, <span class="dt">iterations =</span> <span class="dv">500</span>)</code></pre></div>
<pre><code>## [1] "19 samples filtered by NEATER"</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">filteredNewDataset &lt;-<span class="st"> </span><span class="kw">rbind</span>(newthyroid1, filteredSamples)
<span class="kw"><a href="../reference/plotComparison.html">plotComparison</a></span>(newthyroid1, filteredNewDataset, 
               <span class="dt">attrs =</span> <span class="kw">names</span>(newthyroid1)[<span class="dv">1</span>:<span class="dv">3</span>])</code></pre></div>
<p><img src="imbalance_files/figure-html/example-neater-1.png" width="960"></p>
</div>
<div id="oversampling" class="section level1">
<h1 class="hasAnchor">
<a href="#oversampling" class="anchor"></a>Oversampling</h1>
<div id="mwmote-barua_2014" class="section level2">
<h2 class="hasAnchor">
<a href="#mwmote-barua_2014" class="anchor"></a>MWMOTE <span class="citation">[1]</span>
</h2>
<p>SMOTE is a classic algorithm which generates new examples by filling empty areas among the positive instances. It updates the training set iteratively, by performing:</p>
<p><span class="math display">\[
  E:=E\cup\{x + r(y-x)\}, \quad x,y\in S^{+}, r\sim N(0,1)
\]</span></p>
<p>It has a major setback though: it does not detect noisy instances. Therefore it can generate synthetic examples out of noisy ones or even between two minority classes, which if not cleansed up, may end up becoming noise inside a majority class cluster.</p>
<div class="figure" style="text-align: center">
<img src="smote-flaws.png" alt="SMOTE generating noise" width="60%"><p class="caption">
SMOTE generating noise
</p>
</div>
<p>MWMOTE (<em>Majority Weighted Minority Oversampling Technique</em>) tries to overcome both problems. It intends to give higher weight to borderline instances, undersize minority cluster instances and examples near the borderline of the two clases.</p>
<p>Let us recall the header of the method:</p>
<pre><code><a href="../reference/mwmote.html">mwmote(dataset, numInstances, kNoisy, kMajority, kMinority,
       threshold, cmax, cclustering, classAttr)</a></code></pre>
<p>A KNN algorithm will be used, where we call <span class="math inline">\(d(x,y)\)</span> the euclidean distance between <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>. Let <span class="math inline">\(NN^{k}(x)\subseteq S\)</span> be the <span class="math inline">\(k\)</span>-neighbourhood of <span class="math inline">\(x\)</span> among the whole trainning set (the <span class="math inline">\(k\)</span> closest instances with euclidean distance). Let <span class="math inline">\(NN_{+}^k(x) \subseteq S^{+}\)</span> be its <span class="math inline">\(k\)</span> minority neighbourhood and <span class="math inline">\(NN_{-}^k(x) \subseteq S^{-}\)</span> be its <span class="math inline">\(k\)</span> majority neighbourhood.</p>
<p>For ease of notation, we will name <span class="math inline">\(k_1:=\)</span><code>KNoisy</code>, <span class="math inline">\(k_2:=\)</span><code>KMajority</code>, <span class="math inline">\(k_3:=\)</span><code>KMinority</code>, <span class="math inline">\(\alpha:=\)</span><code>threshold</code>, <span class="math inline">\(C:=\)</span><code>clust</code>, <span class="math inline">\(C_{clust}:=\)</span><code>cclustering</code>.</p>
<p>We define <span class="math inline">\(I_{\alpha,C}(x,y) = C_f(x,y) \cdot D_f(x,y)\)</span>, where if <span class="math inline">\(x \notin NN_{+}^{k_3}(y)\)</span> then <span class="math inline">\(I_{\alpha,C}w(x,y) = 0\)</span>. Otherwise: <span class="math display">\[
  f(x) = \left\{\begin{array}{ll} 
                x &amp;, x\le \alpha \\
                C &amp; \textrm{otherwise}
               \end{array}\right.,\qquad C_f(x,y) = \frac{C}{\alpha} \cdot f\left(\frac{d}{d(x,y)}\right)
\]</span></p>
<p><span class="math inline">\(C_f\)</span> measures the closeness to <span class="math inline">\(y\)</span>, that is, it will measure the proximity of borderline instances.</p>
<p><span class="math inline">\(D_f(x,y) = \frac{C_f(x,y)}{\sum_{z\in V} C_f(z,y)}\)</span> will represent a density factor so an instance belonging to a compact cluster will have higher <span class="math inline">\(\sum C_f(z,y)\)</span> than another one belonging to a more sparse one.</p>
<p>Let <span class="math inline">\(T_{clust}:= C_{clust} \cdot \frac{1}{|S_f^{+}|} \sum_{x\in S_f^{+}} \underset{y\in S_f^{+}, y\neq x}{min} d(x,y)\)</span>. We will also use a mean-average agglomerative hierarchical clustering of the minority instances with threshold <span class="math inline">\(T_{clust}\)</span>, that is, we will use a mean distance: <span class="math display">\[dist(L_i, L_j) = \frac{1}{|L_i||L_j|} \sum_{x\in L_i} \sum_{y\in L_j} d(x,y)\]</span> and having started with a cluster per instance, we will proceed by joining nearest clusters until minimum of distances is lower than <span class="math inline">\(T_{clust}\)</span>.</p>
<p>A general outline of the algorithm is:</p>
<ul>
<li>Firstly, MWMOTE computes a set of filtered positive instances: <span class="math inline">\(S_f^{+}\)</span>, by erasing those instances whose <span class="math inline">\(k_1\)</span>-neighborhood does not contain any positive instance.</li>
<li>Secondly, it computes the positive boundary of <span class="math inline">\(S_f^{+}\)</span>, that is, <span class="math inline">\(U = \cup_{x \in S^{+}_f} NN_{-}^{k_2}(x)\)</span> and the negative boundary, by doing <span class="math inline">\(V = \cup_{x \in U} NN_{+}^{k_3}(x)\)</span>.</li>
<li>For each <span class="math inline">\(x\in V\)</span>, it figures out probability of picking <span class="math inline">\(x\)</span> by assigning: <span class="math inline">\(P(x) = \sum_{y\in U} I_{\alpha, C}(x,y)\)</span> and normalizing those probabilities.</li>
<li>Then, it estimates <span class="math inline">\(L_1, \ldots, L_M\)</span> clusters of <span class="math inline">\(S^{+}\)</span>, with the aforementioned jerarquical agglomerative clustering algorithm and threshold <span class="math inline">\(T_{clust}\)</span>.</li>
<li>Generate <code>numInstances</code> examples by iteratively picking <span class="math inline">\(x\in V\)</span> with respect to probability <span class="math inline">\(P(x)\)</span>, and updating <span class="math inline">\(E:=E\cup \{x+r(y-x)\}\)</span>, where <span class="math inline">\(y\in L_k\)</span> is uniformly picked and <span class="math inline">\(L_k\)</span> is the cluster containing <span class="math inline">\(x\)</span>.</li>
</ul>
<p>A few interesting considerations:</p>
<ul>
<li>Low <span class="math inline">\(k_2\)</span> is required in order to ensure we do not pick too many negative instances in <span class="math inline">\(U\)</span>.</li>
<li>For an opposite reason, a high <span class="math inline">\(k_3\)</span> must be selected to ensure we pick as many positive hard-to-learn borderline examples as we can.</li>
<li>The higher the <span class="math inline">\(C_{clust}\)</span> parameter, the less and more-populated clusters we will get.</li>
</ul>
</div>
<div id="racog-and-wracog-das_2015" class="section level2">
<h2 class="hasAnchor">
<a href="#racog-and-wracog-das_2015" class="anchor"></a>RACOG and wRACOG <span class="citation">[2]</span>
</h2>
<p>These set of algorithms assume we want to approximate a discrete distribution <span class="math inline">\(P(W_1, \ldots, W_d)\)</span>.</p>
<p>Computing that distribution can be too expensive, because we have to compute: <span class="math display">\[
  |\{\textrm{Feasible values for }W_1\}| \cdots |\{\textrm{Feasible values for} W_d\}|
\]</span> total values.</p>
<p>We are going to approximate <span class="math inline">\(P(W_1, \ldots, W_d)\)</span> as <span class="math inline">\(\prod_{i=1}^d P(W_i \mid W_{n(i)})\)</span> where <span class="math inline">\(n(i) \in \{1, \ldots, d\}\)</span>. Chow-Liu’s algorithm will be used to meet that purpose. This algorithm minimizes Kullback-Leibler distance between two distributions: <span class="math display">\[
  D_{KL}(P \parallel Q) = \sum_{i} P(i) \left(\log P(i) - \log Q(i)\right)
\]</span></p>
<p>We recall the definition for the mutual information of two random discrete variables <span class="math inline">\(W_i, W_j\)</span>: <span class="math display">\[
  I(W_i, W_j) = \sum_{w_1\in W_1} \sum_{w_2\in W_2} p(w_1, w_2) \log\left(\frac{p(w_1,w_2)}{p(w_1) p(w_2)}\right)
\]</span></p>
<p>Let <span class="math inline">\(S^{+}=\{x_i = (w_1^{(i)}, \ldots, w_d^{(i)})\}_{i=1}^m\)</span> be the unlabeled positive instances. The algorithm to approximate the distribution that will be used is:</p>
<ul>
<li>Compute <span class="math inline">\(G'=(E',V')\)</span>, Chow Liu’s dependence tree.</li>
<li>If <span class="math inline">\(r\)</span> is the root of the tree, we will define <span class="math inline">\(P(W_r|n(r)):=P(W_r)\)</span>.</li>
<li>For each <span class="math inline">\((u,v) \in E\)</span> arc in the tree, <span class="math inline">\(n(v):=u\)</span> and compute <span class="math inline">\(P(W_v | W_{n(v)})\)</span>.</li>
</ul>
<p>A Gibbs Sampling scheme would later be used to extract samples with respect to the approximated probability distribution, where a badge of new instances is obtained by performing:</p>
<ul>
<li>Given a minority sample <span class="math inline">\(x_k = (w_1^{(i)}, \ldots w_d^{(i)})\)</span>.</li>
<li>Iteratively construct for each attribute <span class="math display">\[\bar{w}_k^{(i)} \sim P(W_k \mid \bar{w}_1^{(i)}, \ldots, \bar{w}_{k-1}^{(i)}, w_{k+1}^{(i)} \ldots, w_{d}^{(i)})\]</span>.</li>
<li>Return <span class="math inline">\(S = \{\bar{x}_i=(\bar{w}_1^{(i)}, \ldots \bar{w}_d^{(i)})\}_{i=1}^m\)</span>.</li>
</ul>
<div class="figure" style="text-align: center">
<img src="monte-carlo.png" alt="Markov chain generated by Gibbs Sampler" width="50%"><p class="caption">
Markov chain generated by Gibbs Sampler
</p>
</div>
<p>Let us recall the headers of <code>racog</code> and <code>wracog</code> functions:</p>
<pre><code><a href="../reference/racog.html">racog(dataset, numInstances, burnin, lag, classAttr)</a></code></pre>
<pre><code><a href="../reference/wracog.html">wracog(train, validation, wrapper, slideWin, 
       threshold, classAttr, ...)</a></code></pre>
<div id="racog" class="section level3">
<h3 class="hasAnchor">
<a href="#racog" class="anchor"></a>RACOG</h3>
<p>RACOG (<em>Rapidly Converging Gibbs</em>) iteratively builds badges of synthetic instances using minority given ones. But it rules out first <code>burnin</code> generated badges and from that moment onwards, it picks a badge of newly-generated examples each <code>lag</code> iterations.</p>
</div>
<div id="wracog" class="section level3">
<h3 class="hasAnchor">
<a href="#wracog" class="anchor"></a>wRACOG</h3>
<p>The downside of RACOG is that it clearly depends on <code>burnin</code>, <code>lag</code> and the requested number of instances <code>numInstances</code>. wRACOG (<em>wrapper-based RACOG</em>) tries to overcome that problem. Let <code>wrapper</code> be a classifier, that could be declared as it follows:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">myWrapper &lt;-<span class="st"> </span><span class="kw">structure</span>(<span class="kw">list</span>(), <span class="dt">class =</span> <span class="st">"C50Wrapper"</span>)
trainWrapper.C50Wrapper &lt;-<span class="st"> </span>function(wrapper, train, trainClass){
  C50::<span class="kw">C5.0</span>(train, trainClass)
}</code></pre></div>
<p>That is, a <code>wrapper</code> should be an <code>S3</code> class with a method <code>trainWrapper</code> following the generic method:</p>
<pre><code><a href="../reference/trainWrapper.html">trainWrapper(wrapper, train, trainClass, ...)</a></code></pre>
<p>Furthermore, the result of <code>trainWrapper</code> must be a <code>predict</code> callable S3 class.</p>
<p>Another example of <code>wrapper</code> with a knn (which can get a little tricky, since it is a lazy classificator):</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(<span class="st">"FNN"</span>)
myWrapper &lt;-<span class="st"> </span><span class="kw">structure</span>(<span class="kw">list</span>(), <span class="dt">class =</span> <span class="st">"KNNWrapper"</span>)

predict.KNN &lt;-<span class="st"> </span>function(model, test){
  FNN::<span class="kw"><a href="http://www.rdocumentation.org/packages/FNN/topics/knn">knn</a></span>(model$train, test, model$trainClass)
}

trainWrapper.KNNWrapper &lt;-<span class="st"> </span>function(wrapper, train, trainClass){
  myKNN &lt;-<span class="st"> </span><span class="kw">structure</span>(<span class="kw">list</span>(), <span class="dt">class =</span> <span class="st">"KNN"</span>)
  myKNN$train &lt;-<span class="st"> </span>train
  myKNN$trainClass &lt;-<span class="st"> </span>trainClass
  myKNN
}</code></pre></div>
<p>where <code>train</code> is the unlabeled tranining dataset, and <code>trainClass</code> are the labels for the training set.</p>
<p>An example of call for this dataset may consist in splitting <code>haberman</code> dataset (provided by the package) into train and validation, and calling wracog with both partitions and any of the aforementioned wrappers:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">data</span>(haberman)

trainFold &lt;-<span class="st"> </span><span class="kw">sample</span>(<span class="dv">1</span>:<span class="kw">nrow</span>(haberman), <span class="kw">nrow</span>(haberman)/<span class="dv">2</span>, <span class="ot">FALSE</span>)
newSamples &lt;-<span class="st"> </span><span class="kw"><a href="../reference/wracog.html">wracog</a></span>(haberman[trainFold, ], haberman[-trainFold, ],
                     myWrapper, <span class="dt">classAttr =</span> <span class="st">"Class"</span>)
<span class="kw">head</span>(newSamples)</code></pre></div>
<pre><code>##   Age Year Positive    Class
## 1  49   64        0 positive
## 2  65   66        2 positive
## 3  50   63        0 positive
## 4  65   66        2 positive
## 5  49   64        0 positive
## 6  65   66        0 positive</code></pre>
</div>
</div>
<div id="rwo-zhang_2014" class="section level2">
<h2 class="hasAnchor">
<a href="#rwo-zhang_2014" class="anchor"></a>RWO <span class="citation">[3]</span>
</h2>
<p>RWO (<em>Random Walk Oversampling</em>) generates synthetic instances so that mean and deviation of numerical attributes remain as close as possible to the original ones. This algorithm is motivated by the central limit theorem.</p>
<div id="central-limit-theorem" class="section level3">
<h3 class="hasAnchor">
<a href="#central-limit-theorem" class="anchor"></a>Central limit theorem</h3>
<p>Let <span class="math inline">\(W_1, \ldots, W_m\)</span> be a collection of independent and identically distributed random variables, with <span class="math inline">\(\mathbb{E}(W_i) = \mu\)</span> and <span class="math inline">\(Var(W_i) = \sigma^2 &lt; \infty\)</span>. Hence: <span class="math display">\[
   \lim_{m} P\left[\frac{\sqrt{m}}{\sigma} \left(\underbrace{\frac{1}{m}\sum_{i=1}^m W_i}_{\overline{W}} - 
   \mu \right) \le z \right] = \phi(z)
 \]</span></p>
<p>where <span class="math inline">\(\phi\)</span> is the distribution function of <span class="math inline">\(N(0,1)\)</span>.</p>
<p>That is, <span class="math inline">\(\frac{\overline{W} - \mu}{\sigma/\sqrt{m}} \rightarrow N(0,1)\)</span> probability-wise.</p>
<p>Let <span class="math inline">\(S^{+}= \{x_i = (w_1^{(i)}, \ldots w_d^{(i)})\}_{i=1}^m\)</span> be the minority instances. Now, let’s fix some <span class="math inline">\(j\in \{1, \ldots d\}\)</span>, and let’s assume that <span class="math inline">\(j\)</span>-ith column follows a numerical random variable <span class="math inline">\(W_j\)</span>, with mean <span class="math inline">\(\mu_j\)</span> and standard deviation <span class="math inline">\(\sigma_j &lt; \infty\)</span>. Let’s compute <span class="math inline">\(\sigma_j' = \sqrt{\frac{1}{m}\sum_{i=1}^m \left(w_j^{(i)} - \frac{\sum_{i=1}^m w_j^{(i)}}{m} \right)^2}\)</span> the biased estimator for the standard deviation. It can be proven that instances generated with <span class="math inline">\(\bar{w}_j = w_j^{(i)} - \frac{\sigma_j'}{\sqrt{m}}\cdot r, r\sim N(0,1)\)</span> have the same sample mean as the original ones, and their sample variance tends to the original one.</p>
</div>
<div id="outline-of-the-algorithm" class="section level3">
<h3 class="hasAnchor">
<a href="#outline-of-the-algorithm" class="anchor"></a>Outline of the algorithm</h3>
<p>Our algorithm will proceed as follows:</p>
<ul>
<li>For each numerical attribute <span class="math inline">\(j=1, \ldots, d\)</span> compute the standard deviation of the column, <span class="math inline">\(\sigma_j' = \sqrt{\frac{1}{m}\sum_{i=1}^m \left(w_j^{(i)} - \frac{\sum_{i=1}^m w_j^{(i)}}{m} \right)^2}\)</span>.</li>
<li>For a given instance <span class="math inline">\(x_i=(w_1^{(i)}, \ldots, w_d^{(i)})\)</span>, for each attribute attribute <span class="math inline">\(j\)</span>, generate:</li>
</ul>
<p><span class="math display">\[
\bar{w}_j = \left\{\begin{array}{ll}
w_j^{(i)} - \frac{\sigma_j'}{\sqrt{m}}\cdot r, r\sim N(0,1) &amp; \textrm{if numerical attribute}\\
\textrm{pick uniformly over } \{w_j^{(1)}, \ldots w_j^{(m)}\} &amp; \textrm{otherwise}
\end{array}\right.
\]</span></p>
</div>
</div>
<div id="pdfos--gao_2014" class="section level2">
<h2 class="hasAnchor">
<a href="#pdfos--gao_2014" class="anchor"></a>PDFOS <span class="citation">[4]</span>
</h2>
<div id="motivation" class="section level3">
<h3 class="hasAnchor">
<a href="#motivation" class="anchor"></a>Motivation</h3>
<p>Given a distribution function of a random variable <span class="math inline">\(X\)</span>, namely <span class="math inline">\(F(x)\)</span>, if that function has an almost everywhere derivative, then, almost everywhere, it holds: <span class="math display">\[
  f(x) = \lim_{h\rightarrow 0} \frac{F(x+h) - F(x-h)}{2h} = \lim_{h\rightarrow 0} \frac{P(x-h &lt; X \le x+h)}{2h}
\]</span></p>
<p>Given random samples of <span class="math inline">\(X\)</span>, <span class="math inline">\(X_1, \ldots X_n\)</span>, namely <span class="math inline">\(x_1, \ldots x_n\)</span>, an estimator for <span class="math inline">\(f\)</span> could be the mean of samples in <span class="math inline">\(]x-h, x+h[\)</span> divided by the length of the interval: <span class="math display">\[
  \widehat{f}(x) = \frac{1}{2hn} \bigg[\textrm{Number of samples } x_1, \ldots, x_n \textrm{ that belong to ]x-h, x+h[}\bigg]
\]</span></p>
<p>If we define <span class="math inline">\(\omega(x) = \left\{\begin{array}{ll}  \frac{1}{2} &amp;, |x| &lt; 1\\  0 &amp; \textrm{otherwise}  \end{array}\right.\)</span></p>
<p>and <span class="math inline">\(w_h(x) = w\left(\left|\frac{x}{h}\right|\right)\)</span>, then we could write <span class="math inline">\(\widehat{f}\)</span> as: <span class="math display">\[
  \widehat{f}(x) = \frac{1}{nh} \sum_{i=1}^n \omega_h(x-x_i)
\]</span></p>
<p>It we assume that <span class="math inline">\(x_1, \ldots, x_n\)</span> are equidistant with distance <span class="math inline">\(2h\)</span> (they are placed in the middle of <span class="math inline">\(2h\)</span> length intervals), <span class="math inline">\(\widehat{f}\)</span> could be seen as an histogram where each bar has a <span class="math inline">\(2h\)</span> width and a <span class="math inline">\(\frac{1}{2nh} \cdot \bigg[[\textrm{Number of samples } x_1, \ldots, x_n \textrm{ that belong to the interval}]\bigg]\)</span> length. Parameter <span class="math inline">\(h\)</span> is called <em>bandwidth</em>.</p>
<p>In multivariate case (<span class="math inline">\(d\)</span> dimensional), we define: <span class="math display">\[
  \widehat{f}(x) = \frac{1}{nh^d} \sum_{i=1}^n \omega_h(x-x_i)
\]</span></p>
</div>
<div id="kernel-methods" class="section level3">
<h3 class="hasAnchor">
<a href="#kernel-methods" class="anchor"></a>Kernel methods</h3>
<p>If we took <span class="math inline">\(w = \frac{1}{2} \Large{1}\normalsize_{]-1,1[}\)</span>, then <span class="math inline">\(\widehat{f}\)</span> would have jump discontinuities and we would have jump derivatives. On the other hand, we could took <span class="math inline">\(\omega\)</span>, where <span class="math inline">\(w\ge 0\)</span>, <span class="math inline">\(\int_{\Omega} \omega(x) dx = 1\)</span>, <span class="math inline">\(\Omega \subseteq X\)</span> a domain, and <span class="math inline">\(w\)</span> were even, and that way we could have estimators with more desirable properties with respect to continuity and differentiability.</p>
<p><span class="math inline">\(\widehat{f}\)</span> can be evaluated through its MISE (<em>Mean Integral Squared Error</em>): <span class="math display">\[
  MISE(h) = \underset{x_1, \ldots, x_d}{\mathbb{E}} \int (\widehat{f}(x) - f(x))^2 dx
\]</span></p>
<div class="figure" style="text-align: center">
<img src="kernel-estimation.png" alt="Example of kernel estimation" width="75%"><p class="caption">
Example of kernel estimation
</p>
</div>
<div id="gaussian-kernels" class="section level4">
<h4 class="hasAnchor">
<a href="#gaussian-kernels" class="anchor"></a>Gaussian kernels</h4>
<p>PDFOS (<em>Probability Distribution density Function estimation based Oversampling</em>) uses multivariate Gaussian kernel methods. The probability density function of a <span class="math inline">\(d\)</span>-Gaussian distribution with mean <span class="math inline">\(0\)</span> and <span class="math inline">\(\Psi\)</span> as its covariance matrix is: <span class="math display">\[
  \phi^{\Psi}(x) = \frac{1}{\sqrt{(2\pi \cdot det(\Psi))^d}} exp\left(-\frac{1}{2} x \Psi^{-1} x^T \right)
\]</span></p>
<p>Let <span class="math inline">\(S^{+} = \{x_i = (w_1^{(i)}, \ldots, w_d^{(i)})\}_{i=1}^m\)</span> be the minority instances. The unbiased covariance estimator is: <span class="math display">\[
  U = \frac{1}{m-1} \sum_{i=1}^m (x_i - \overline{x})(x_i - \overline{x})^T, 
  \qquad \textrm{where } \overline{x} = \frac{1}{m}\sum_{i=1}^m x_i
\]</span></p>
<p>We will use kernel functions <span class="math inline">\(\phi_h(x) = \phi^U\left(\frac{x}{h}\right)\)</span>, where <span class="math inline">\(h\)</span> ought to be optimized to minimize the MISE. It is well-known that can be achieved by minimizing the following cross validation function: <span class="math display">\[
 M(h) = \frac{1}{m^2 h^d} \sum_{i=1}^m \sum_{j=1}^m \phi_h^{\ast} (x_i - x_j) + \frac{2}{m h^d} \phi_h(0)
\]</span> where <span class="math inline">\(\phi_h^{\ast} \approx \phi_{h\sqrt{2}} - 2\phi_h\)</span>.</p>
<p>Once a proper <span class="math inline">\(h\)</span> has been found, a suitable generating scheme could be to take <span class="math inline">\(x_i + h R r\)</span>, where <span class="math inline">\(x_i \in S^{+}\)</span>, <span class="math inline">\(r\sim N^d(0,1)\)</span> and <span class="math inline">\(U = R\cdot R^T\)</span>. In case we have enough guarantees to decompose <span class="math inline">\(U = R^T \cdot R\)</span> (<span class="math inline">\(U\)</span> must be a positive-definite matrix), we could use Choleski decomposition. In fact, we provide a sketch of proof showing that all covariance matrices are positive-semidefinite: <span class="math display">\[
y^T \left(\sum_{i=1}^m (x_i - \overline{x})(x_i - \overline{x})^T\right) y = \sum_{i=1}^m (\underbrace{(x_i - \overline{x})^T y}_{z_i^T})^T \underbrace{(x_i - \overline{x})^T y}_{z_i}) = \sum_{i=1}^m ||z_i||^2\ge 0
\]</span> for arbitrary <span class="math inline">\(y \in \mathbb{R}^d\)</span>. We need a strict positive definite matrix, otherwise PDFOS would not provide a result and will stop its execution.</p>
</div>
</div>
<div id="search-of-optimal-bandwidth" class="section level3">
<h3 class="hasAnchor">
<a href="#search-of-optimal-bandwidth" class="anchor"></a>Search of optimal bandwidth</h3>
<p>We take a first approximation to <span class="math inline">\(h\)</span> as the value: <span class="math display">\[
  h_{Silverman} = \left(\frac{4}{m(d+2)}\right)^{\frac{1}{d+4}}
\]</span> where <span class="math inline">\(d\)</span> is number of attributes and <span class="math inline">\(m\)</span> the size of the minority class.</p>
Reshaping the equation of the cross validation function and differentiating:
\begin{align}
M(h) &amp;= \frac{1}{m^2 h^d} \sum_{i=1}^m \sum_{j=1}^m \phi_h^{\ast} (x_i - x_j) + \frac{2}{m h^d} \phi_h(0) \nonumber\\
     &amp;= \frac{1}{m^2 h^d} \sum_{i=1}^m \sum_{j=1, j\neq i}^m \phi_h^{\ast} (x_i - x_j) + \frac{1}{m h^{d}} \phi_{h\sqrt{2}}(0) \nonumber\\
     &amp;= \frac{2}{m^2 h^d} \sum_{j &gt; i}^m \phi_h^{\ast} (x_i - x_j) + \frac{1}{m h^{d}} \phi_{h\sqrt{2}}(0)
 \label{eq:cv-simp}
\end{align}
\begin{align}
\frac{\partial M}{\partial h}(h) &amp;= \frac{2}{m^2 h^d} \sum_{j&gt;i}^m  \phi_h^{\ast} (x_i - x_j)
 \bigg(-d h^{-1} + h^{-3} (x_i-x_j)^T U (x_i-x_j) \bigg) \nonumber
 - \frac{dh^{-1}}{mh^{d}} \phi_{h\sqrt{2}}(0)
 \label{eq:cross-val-df}
\end{align}
<p>And a straightforward <em>gradient descendent</em> algorithm is used to find a good <span class="math inline">\(h\)</span> estimation.</p>
</div>
</div>
</div>
<div id="filtering" class="section level1">
<h1 class="hasAnchor">
<a href="#filtering" class="anchor"></a>Filtering</h1>
<p>Once we have created synthetic examples, we should ask ourselves how many of those instances are in fact relevant to our problem. Filtering algorithms can be applied to <em>oversampled</em> datasets, to erase the least relevant instances.</p>
<div id="neater-almogahed_2014" class="section level2">
<h2 class="hasAnchor">
<a href="#neater-almogahed_2014" class="anchor"></a>NEATER <span class="citation">[5]</span>
</h2>
<p>NEATER (<em>filteriNg of ovErsampled dAta using non cooperaTive gamE theoRy</em>) is a filtering algorithm based on game theory.</p>
<div id="introduction-to-game-theory" class="section level3">
<h3 class="hasAnchor">
<a href="#introduction-to-game-theory" class="anchor"></a>Introduction to Game Theory</h3>
<p>Let <span class="math inline">\((P, T, f)\)</span> be our game space. We would have a set of players, <span class="math inline">\(P=\{1, \ldots, n\}\)</span>, and <span class="math inline">\(T_i=\{1, \ldots, k_i\}\)</span>, set of feasible strategies for the <span class="math inline">\(i\)</span>-th player, resulting in <span class="math inline">\(T = T_1 \times \ldots \times T_n\)</span>. We can easily assign a payoff to each player taking into account his/her own strategy as well as other players’ strategy. So <span class="math inline">\(f\)</span> will be given by the following equation: <span class="math display">\[
  \begin{array}{rll}
  f: T &amp;\longrightarrow&amp; \mathbb{R}^n\\
  t &amp;\longmapsto&amp; (f_1(t), \ldots, f_n(t))
 \end{array}
\]</span></p>
<p><span class="math inline">\(t_{-i}\)</span> will denote <span class="math inline">\((t_1, \ldots, t_{i-1}, t_{i+1}, \ldots, t_n)\)</span> and similarly we can denote <span class="math inline">\(f_i(t_i, t_{-i})= f_i(t)\)</span>.</p>
<p>An <em>strategic Nash equilibrium</em> is a tuple <span class="math inline">\((t_1, \ldots, t_n)\)</span> where <span class="math inline">\(f_i(t_i, t_{-i}) \ge f_i(t'_{i}, t_{-i})\)</span> for every other <span class="math inline">\(t'\in T\)</span>, and all <span class="math inline">\(i=1, \ldots, n\)</span>. That is, an strategic Nash equilibrium maximizes the payoff for all the players.</p>
<p>The strategy for each player will be picked with respect to a given probability: <span class="math display">\[
\delta_i \in \Delta_i = \{(\delta_i^{(1)}, \ldots, \delta_i^{(k_i)}) \in (R^{+}_0)^{k_i} : \sum_{j=1}^{k_i} \delta_i^{(j)} = 1\}
\]</span> We define <span class="math inline">\(\Delta_1 \times \ldots \times \Delta_n := \Delta\)</span> and we call an element <span class="math inline">\(\delta = (\delta_1, \ldots, \delta_n) \in \Delta\)</span> an strategy profile. Having a fixed strategy profile <span class="math inline">\(\delta\)</span>, the overall payoff for the <span class="math inline">\(i\)</span>-th player is defined as: <span class="math display">\[
  u_i(\delta) = \sum_{(t_1, \ldots, t_n)\in T} \delta_i^{(t_i)} f_i(t)
\]</span></p>
<p>Given <span class="math inline">\(u_i\)</span> the payoff for a <span class="math inline">\(\delta\)</span> strategy profile in the <span class="math inline">\(i\)</span>-th player and <span class="math inline">\(\delta\in \Delta\)</span> we will denote</p>
\begin{align}
\delta_{-i} &amp;:= (\delta_1, \ldots, \delta_{i-1}, \delta_{i+1}, \ldots, \delta_n)\\
u_i(\delta_i, \delta_{-i}) &amp;:= u_i(\delta)
\end{align}
<p>A <em>probabilistic Nash equilibrium</em> is a strategy profile <span class="math inline">\(x = (\delta_1, \ldots, \delta_n)\)</span> verifying <span class="math inline">\(u_i(\delta_i, \delta_{-i}) \ge u_i(\delta'_{i}, \delta_{-i})\)</span> for every other <span class="math inline">\(\delta'\in \Delta\)</span>, and all <span class="math inline">\(i=1, \ldots, n\)</span>.</p>
<p>A theorem ensures that every game space <span class="math inline">\((P,T,f)\)</span> with finite players and strategies has a <em>probabistic Nash equilibrium</em></p>
</div>
<div id="particularization-to-imbalance-problem" class="section level3">
<h3 class="hasAnchor">
<a href="#particularization-to-imbalance-problem" class="anchor"></a>Particularization to imbalance problem</h3>
<p>Let <span class="math inline">\(S\)</span> be the original training set, <span class="math inline">\(E\)</span> the synthetic generated instances. Our players would be <span class="math inline">\(S \cup E\)</span>. Every player would be able to pick between two different strategies: <span class="math inline">\(0\)</span> - being a negative instance - and <span class="math inline">\(1\)</span> - being a positive instance -. Players of <span class="math inline">\(S\)</span> would always have a fixed strategy, where the <span class="math inline">\(i\)</span>-th player would have <span class="math inline">\(\delta_i = (0,1)\)</span> (a <span class="math inline">\(0\)</span> strategy) in case it is a negative instance or <span class="math inline">\(\delta_i = (1,0)\)</span> (a <span class="math inline">\(1\)</span> strategy) otherwise.</p>
<p>The payoff for a given instance is affected only by its own strategy and its <span class="math inline">\(k\)</span> nearest neighbors in <span class="math inline">\(S\cup E\)</span>. That is, for every <span class="math inline">\(x_i \in E\)</span>, we will have <span class="math inline">\(u_i(\delta) = \sum_{j\in NN^k(x)} (x_i^T w_{ij} x_j)\)</span> where <span class="math inline">\(w_{ij} = g\left(d(x_i, x_j)\right)\)</span> and <span class="math inline">\(g\)</span> is a decreasing function (the further, the lower payoff). In our implementation, we have considered <span class="math inline">\(g(z) = \frac{1}{1+z^2}\)</span>, with <span class="math inline">\(d\)</span> the euclidean distance.</p>
Each step should involve an update to the strategy profiles of instances of <span class="math inline">\(E\)</span>. Namely, if <span class="math inline">\(x_i \in E\)</span>, the following equation will be used:
\begin{align*}
&amp; \delta_i(0) = \left(\frac{1}{2}, \frac{1}{2}\right)\\
&amp; \delta_{i,1}(n+1) = \frac{\alpha + u_i((1,0))}{\alpha + u_i(\delta(n))} \delta_{i,1}(n)\\
&amp; \delta_{i,2}(n+1) = 1 - \delta_{i,1}(n+1)
\end{align*}
<p>That is, we are reinforcing the strategy that is producing the higher payoff, in detriment to the opposite strategy. This method has enough convergence guarantees.</p>
<p>Let’s recall the header for <code>neater</code>:</p>
<pre><code><a href="../reference/neater.html">neater(dataset, newSamples, k, iterations, smoothFactor, classAttr)</a></code></pre>
<p>Then a rough sketch of the algorithm is:</p>
<ul>
<li>Compute <code>k</code> nearest neighbors for every instance of <span class="math inline">\(E:=\)</span><code>newSamples</code>.</li>
<li>Initialize strategy profiles of <code>dataset</code><span class="math inline">\(\cup\)</span><code>newSamples</code>.</li>
<li>Iterate <code>iterations</code> times updating payoffs with the aforementioned rule and strategy profiles.</li>
<li>Keep only those examples of <code>newSamples</code> with probability of being positive instances higher than <span class="math inline">\(0.5\)</span>.</li>
</ul>
</div>
</div>
</div>
<div id="references" class="section level1 unnumbered">
<h1 class="hasAnchor">
<a href="#references" class="anchor"></a>References</h1>
<div id="refs" class="references">
<div id="ref-Barua_2014">
<p>[1] <span style="font-variant: small-caps;">Barua</span>, S., <span style="font-variant: small-caps;">Islam</span>, M. M., <span style="font-variant: small-caps;">Yao</span>, X. and <span style="font-variant: small-caps;">Murase</span>, K. (2014). MWMOTE–Majority weighted minority oversampling technique for imbalanced data set learning. <em>IEEE Transactions on Knowledge and Data Engineering</em> <strong>26</strong> 405–25.</p>
</div>
<div id="ref-Das_2015">
<p>[2] <span style="font-variant: small-caps;">Das</span>, B., <span style="font-variant: small-caps;">Krishnan</span>, N. C. and <span style="font-variant: small-caps;">Cook</span>, D. J. (2015). RACOG and wRACOG: Two probabilistic oversampling techniques. <em>IEEE Transactions on Knowledge and Data Engineering</em> <strong>27</strong> 222–34.</p>
</div>
<div id="ref-Zhang_2014">
<p>[3] <span style="font-variant: small-caps;">Zhang</span>, H. and <span style="font-variant: small-caps;">Li</span>, M. (2014). RWO-sampling: A random walk over-sampling approach to imbalanced data classification. <em>Information Fusion</em> <strong>20</strong> 99–116.</p>
</div>
<div id="ref-Gao_2014">
<p>[4] <span style="font-variant: small-caps;">Gao</span>, M., <span style="font-variant: small-caps;">Hong</span>, X., <span style="font-variant: small-caps;">Chen</span>, S., <span style="font-variant: small-caps;">Harris</span>, C. J. and <span style="font-variant: small-caps;">Khalaf</span>, E. (2014). PDFOS: PDF estimation based over-sampling for imbalanced two-class problems. <em>Neurocomputing</em> <strong>138</strong> 248–59.</p>
</div>
<div id="ref-Almogahed_2014">
<p>[5] <span style="font-variant: small-caps;">Almogahed</span>, B. A. and <span style="font-variant: small-caps;">Kakadiaris</span>, I. A. (2014). NEATER: Filtering of over-sampled data using non-cooperative game theory. <em>Soft Computing</em> <strong>19</strong> 3301–22.</p>
</div>
</div>
</div>
</div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="sidebar">
        <div id="tocnav">
      <h2 class="hasAnchor">
<a href="#tocnav" class="anchor"></a>Contents</h2>
      <ul class="nav nav-pills nav-stacked">
<li><a href="#imbalance-classification-problem">Imbalance classification problem</a></li>
      <li><a href="#contents-of-the-package">Contents of the package</a></li>
      <li>
<a href="#oversampling">Oversampling</a><ul class="nav nav-pills nav-stacked">
<li><a href="#mwmote-barua_2014">MWMOTE <span class="citation">[1]</span></a></li>
      <li><a href="#racog-and-wracog-das_2015">RACOG and wRACOG <span class="citation">[2]</span></a></li>
      <li><a href="#rwo-zhang_2014">RWO <span class="citation">[3]</span></a></li>
      <li><a href="#pdfos--gao_2014">PDFOS <span class="citation">[4]</span></a></li>
      </ul>
</li>
      <li>
<a href="#filtering">Filtering</a><ul class="nav nav-pills nav-stacked">
<li><a href="#neater-almogahed_2014">NEATER <span class="citation">[5]</span></a></li>
      </ul>
</li>
      <li><a href="#references">References</a></li>
      </ul>
</div>
      </div>

</div>


      <footer><div class="copyright">
  <p>Developed by Ignacio Cordón, Salvador García, Alberto Fernández, Francisco Herrera.</p>
</div>

<div class="pkgdown">
  <p>Site built with <a href="http://pkgdown.r-lib.org/">pkgdown</a>.</p>
</div>

      </footer>
</div>

  </body>
</html>
